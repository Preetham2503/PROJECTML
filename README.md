# Multimodal Skin Lesion Classification

> Early melanoma detection through multimodal learning over dermatoscopic imagery and patient metadata.

This repository implements an end-to-end pipeline for the **HAM10000** dataset, unifying dermatoscopic image embeddings and structured metadata to perform both **7-class lesion diagnosis** and **binary malignant vs. benign screening**. The project covers preprocessing, feature extraction, classical and neural modeling, evaluation, interpretability, and reporting, all reproducibly orchestrated via CLI scripts.

---

## üìÅ Project Structure

```
‚îú‚îÄ‚îÄ DATASET/                 # HAM10000 images & metadata CSV
‚îú‚îÄ‚îÄ artifacts/               # Generated plots, metrics, logs, cached embeddings
‚îÇ   ‚îú‚îÄ‚îÄ eda/                 # Exploratory data analysis visuals
‚îÇ   ‚îú‚îÄ‚îÄ metrics/             # CSV summaries, bar/radar charts, significance tests
‚îÇ   ‚îú‚îÄ‚îÄ roc_curves/          # ROC curves for each model
‚îÇ   ‚îú‚îÄ‚îÄ pr_curves/           # Precision-recall curves
‚îÇ   ‚îú‚îÄ‚îÄ gradcam/             # Grad-CAM heatmaps
‚îÇ   ‚îú‚îÄ‚îÄ shap/                # SHAP plots & feature importances
‚îÇ   ‚îî‚îÄ‚îÄ logs/                # env_info.json, pip_freeze.txt, preprocessing logs
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îú‚îÄ‚îÄ project_config.json  # Global config (e.g., seeds)
‚îÇ   ‚îî‚îÄ‚îÄ seed_config.json     # Deterministic random seed definitions
‚îú‚îÄ‚îÄ data/                    # Train/val/test CSV splits (multiclass & binary)
‚îú‚îÄ‚îÄ models/                  # Saved models (.joblib, .pt) and cached features (.npz)
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ 01_end_to_end.ipynb  # Guided notebook walkthrough
‚îú‚îÄ‚îÄ scripts/                 # CLI tooling for preprocessing, training, evaluation
‚îú‚îÄ‚îÄ utils/                   # Shared helpers (metrics, SHAP, Grad-CAM, late fusion, seeds)
‚îú‚îÄ‚îÄ .venv/                   # Local virtual environment (not tracked in VCS)
‚îú‚îÄ‚îÄ requirements.txt         # Pinned Python dependencies
‚îú‚îÄ‚îÄ setup_venv.sh            # Convenience script to create & populate .venv
‚îî‚îÄ‚îÄ README.md
```

---

## üõ†Ô∏è Environment Setup

```bash
# (Optional) create .venv using the helper script
chmod +x setup_venv.sh
./setup_venv.sh          # runs: python3 -m venv .venv && pip install -r requirements.txt

# activate the environment
source .venv/bin/activate

# or manually install if preferred
pip install -r requirements.txt
```

**Pinned stack:** Python ‚â• 3.11 (tested), PyTorch 2.4.0, torchvision 0.19.0, scikit-learn 1.5.2, pandas 2.2.3, numpy 1.26.4, shap 0.45.0, seaborn 0.13.2, matplotlib 3.9.2, OpenCV 4.10, tqdm 4.66.5.

---

## üì¶ Dataset Placement

1. Download HAM10000 from the official source (ISIC archive / Kaggle). The Kaggle mirror is available here: [Skin Cancer MNIST: HAM10000](https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000).  
2. Place the files as follows:

```
DATASET/
‚îÇ‚îÄ‚îÄ HAM10000_metadata.csv
‚îÇ‚îÄ‚îÄ HAM10000_images_part_1/
‚îÇ   ‚îî‚îÄ‚îÄ *.jpg
‚îî‚îÄ‚îÄ HAM10000_images_part_2/
    ‚îî‚îÄ‚îÄ *.jpg
```

3. `scripts/preprocess_data.py` resolves absolute paths to these directories. If you use a different layout, edit the `DATASET_ROOT` constants within that script.

---

## üöÄ Run the Pipeline

> Activate the virtual environment before executing any CLI script.

```bash
source .venv/bin/activate

# 1. Generate splits, binary labels, diagnostics
python scripts/preprocess_data.py

# 2. Train image-only classical models on ResNet50 embeddings
python scripts/train_image_only.py

# 3. Train metadata-only classical models
python scripts/train_metadata_only.py

# 4. Train fusion models (classical + Fusion MLP)
python scripts/train_fusion.py

# 5. (Optional) regenerate CNN embeddings (e.g., switch backbone)
python scripts/extract_embeddings.py --model resnet50 --batch_size 32

# 6. Comprehensive evaluation, interpretability, subgroup analysis
python scripts/evaluate.py --debug

# Optional utilities
python scripts/run_weighted_late_fusion.py          # weighted ensemble experiments
python scripts/run_binary_significance.py          # McNemar & Friedman tests
python scripts/run_shap_extended.py                # extended SHAP waterfall plots
python scripts/run_image_pca_update.py             # PCA refresh + retraining
python scripts/aggregate_results.py                # leaderboard + visual summaries
```

---

## üìì Notebook Companion

- **`notebooks/01_end_to_end.ipynb`** provides a narrative walkthrough covering preprocessing summaries, sample visualizations, and model performance snapshots. It‚Äôs ideal for newcomers wanting an interactive tour before diving into the CLI workflow.

---

## üóÇÔ∏è Outputs & Artifacts

- `artifacts/eda/` ‚Äì Class distributions, age/localization plots, correlation heatmaps, PCA variance & CI charts.  
- `artifacts/evaluation/multiclass/` ‚Äì Confusion matrices, ROC/PR curves, SHAP plots, subgroup analyses, Grad-CAM galleries, and metrics for the seven-class task.  
- `artifacts/evaluation/binary/` ‚Äì Analogous artifacts for the malignant-vs-benign task, including weighted late-fusion outputs and statistical tests.  
- `artifacts/evaluation/summary/` ‚Äì Aggregated leaderboards (`model_comparison.csv`, bar/radar charts) spanning both tasks.  
- `artifacts/eda/` ‚Äì Exploratory data analysis visuals (class balance, correlation heatmaps, PCA variance, etc.).  
- `artifacts/cache/` ‚Äì Cached image embeddings for train/val/test splits (ResNet50, VGG16 variants).  
- `artifacts/logs/` ‚Äì `env_info.json`, `pip_freeze.txt`, preprocess/train/evaluate logs for reproducibility.

Example interpretability artifact:

![Grad-CAM example](artifacts/gradcam/grad_cam_tp_1.png)

---

## üîÅ Reproducibility

- **Seeds:** All scripts load deterministic seeds from `configs/seed_config.json` / `configs/project_config.json`.  
- **Environment Logs:** `artifacts/logs/env_info.json` captures Python, PyTorch, scikit-learn versions; `pip_freeze.txt` snapshots installed packages after setup.  
- **Configurable CLI:** Every script exposes arguments (`--data-dir`, `--models-dir`, etc.) to tailor experiments while maintaining a traceable command history.

---

## üìö Citations & References

- **Dataset:** Tschandl, Rosendahl, and Kittler. ‚ÄúThe HAM10000 dataset: A large collection of multi-source dermatoscopic images of common pigmented skin lesions.‚Äù *Scientific Data*, 2018.  
- **CNN Backbone:** Kaiming He et al. ‚ÄúDeep Residual Learning for Image Recognition.‚Äù *CVPR*, 2016 (ResNet50).  
- **Interpretability:** Lundberg & Lee. ‚ÄúA Unified Approach to Interpreting Model Predictions.‚Äù *NeurIPS*, 2017 (SHAP).  
- **Grad-CAM:** Selvaraju et al. ‚ÄúGrad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization.‚Äù *ICCV*, 2017.

---


